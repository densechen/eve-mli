{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eve: Make Deep Learning More Interesting\n",
    "\n",
    "Before starting, please make sure **Eve** is in your python path.\n",
    "\n",
    "You can install **Eve** via PyPi by ```pip install eve-ml```, \n",
    "\n",
    "then check it out by ```python -c \"import eve; print(eve.__version__)\"```.\n",
    "\n",
    "Now, let's learn more about **Eve**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eve is a native extension of PyTorch\n",
    "\n",
    "The core module of Eve is **eve.cores.Eve**, which succeed **torch.nn.Module**.\n",
    "\n",
    "You can build a deep learning network with **Eve** just like what you used to do with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eve\n",
    "import torch\n",
    "import eve.cores\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a toy model using **Eve** to solve MNIST classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use new features of Eve, you must ensure the network succeed Eve, not nn.Module\n",
    "class ToyModel(eve.cores.Eve):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # You can define any nn.Module in Eve, Eve can handle them well.\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=2, padding=1),\n",
    "            torch.nn.BatchNorm2d(3),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.linear = torch.nn.Linear(14 * 14 * 3, 10)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        conv = self.conv(x)\n",
    "        \n",
    "        conv = torch.flatten(conv, 1)\n",
    "        linear = self.linear(conv)\n",
    "        return torch.log_softmax(linear, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and define dataloader. **data_root** is what the MNIST dataset will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/media/densechen/data/dataset\"\n",
    "\n",
    "# define dataset\n",
    "train_dataset = MNIST(root=data_root, train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = MNIST(root=data_root, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# define dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, num_workers=2, batch_size=128, drop_last=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, num_workers=2, batch_size=128, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network and optimizer, and move them to cuda if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda\n"
     ]
    }
   ],
   "source": [
    "# define network\n",
    "toy_model = ToyModel()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    toy_model.cuda()\n",
    "    print(\"Use cuda\")\n",
    "else:\n",
    "    print(\"Use cpu\")\n",
    "    \n",
    "# define optimzier\n",
    "# !WARN: in any case, you should call model.torch_parameters() to gather the parameters needed for optimizer.\n",
    "optimizer = torch.optim.Adam(toy_model.torch_parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train toy_model in dataset for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc on Epoch 1/10 is 91.53%\n",
      "Acc on Epoch 2/10 is 93.00%\n",
      "Acc on Epoch 3/10 is 93.71%\n",
      "Acc on Epoch 4/10 is 94.20%\n",
      "Acc on Epoch 5/10 is 94.94%\n",
      "Acc on Epoch 6/10 is 95.20%\n",
      "Acc on Epoch 7/10 is 95.58%\n",
      "Acc on Epoch 8/10 is 95.81%\n",
      "Acc on Epoch 9/10 is 96.05%\n",
      "Acc on Epoch 10/10 is 96.27%\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    toy_model.train()\n",
    "    for data, target in train_dataloader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.nn.functional.cross_entropy(toy_model(data), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    correct = 0.0\n",
    "    toy_model.eval()\n",
    "    for data, target in test_dataloader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        predict = toy_model(data)\n",
    "        correct += (predict.max(dim=1)[1] == target).float().sum()\n",
    "    print(f\"Acc on Epoch {i+1}/10 is {100 * correct/len(test_dataset):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Neural Network\n",
    "\n",
    "**Eve** supports many quantization methods, which is developed recently.\n",
    "\n",
    "Now, let's design a STE quantization model to solve the MNIST classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use new features of Eve, you must ensure the network succeed Eve, not nn.Module\n",
    "class SteToyModel(eve.cores.Eve):\n",
    "    def __init__(self, max_bits=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # You can define any nn.Module in Eve, Eve can handle them well.\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=2, padding=1),\n",
    "            torch.nn.BatchNorm2d(3),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        state = eve.cores.State(self.conv)\n",
    "        self.ste = eve.cores.SteQuan(state=state, max_bits=max_bits)\n",
    "        \n",
    "        self.linear = torch.nn.Linear(14 * 14 * 3, 10)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        conv = self.conv(x)\n",
    "        \n",
    "        ste = self.ste(conv)\n",
    "        \n",
    "        ste = torch.flatten(ste, 1)\n",
    "        linear = self.linear(ste)\n",
    "        return torch.log_softmax(linear, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an STE toy model and optimizer, and move them to cuda if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda\n"
     ]
    }
   ],
   "source": [
    "# define network\n",
    "ste_toy_model = SteToyModel(max_bits=4)\n",
    "\n",
    "if use_cuda:\n",
    "    ste_toy_model.cuda()\n",
    "    print(\"Use cuda\")\n",
    "else:\n",
    "    print(\"Use cpu\")\n",
    "    \n",
    "# define optimzier\n",
    "optimizer = torch.optim.Adam(ste_toy_model.torch_parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ste_toy_model in dataset for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc on Epoch 1/10 is 89.51%\n",
      "Acc on Epoch 2/10 is 91.11%\n",
      "Acc on Epoch 3/10 is 91.38%\n",
      "Acc on Epoch 4/10 is 91.71%\n",
      "Acc on Epoch 5/10 is 91.67%\n",
      "Acc on Epoch 6/10 is 91.59%\n",
      "Acc on Epoch 7/10 is 91.68%\n",
      "Acc on Epoch 8/10 is 91.61%\n",
      "Acc on Epoch 9/10 is 91.55%\n",
      "Acc on Epoch 10/10 is 91.33%\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    ste_toy_model.train()\n",
    "    for data, target in train_dataloader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.nn.functional.cross_entropy(ste_toy_model(data), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    correct = 0.0\n",
    "    ste_toy_model.eval()\n",
    "    for data, target in test_dataloader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        predict = ste_toy_model(data)\n",
    "        correct += (predict.max(dim=1)[1] == target).float().sum()\n",
    "    print(f\"Acc on Epoch {i+1}/10 is {100 * correct/len(test_dataset):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use a max_bits = 4 to do the quantization operation, and we find that by quantization, the \n",
    "performance of model can be further improved. This is also a special property of Quantization.\n",
    "\n",
    "We think that the quantization operation is benifit for improving generalization ability of model compared \n",
    "with full-precision network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiking Neural Network\n",
    "\n",
    "**Eve** designs some special attributes to support hidden states rather than **torch.nn.Buffer**.\n",
    "\n",
    "The hidden states of **Eve** will reset while calling **Eve.reset()** and will not be fetched by state_dict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnnToyModel(eve.cores.Eve):\n",
    "    def __init__(self, timesteps=5):\n",
    "        \"\"\"Different with traditional neural network, spiking neural network should repeat many times for a specifed \n",
    "        input image.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # recored the max timesteps\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        # For spiking neural network, you should specified a encoder for it to transfer the rate encoding to\n",
    "        # spiking trains. Here, we use the possion encoder just likes many spiking neural networks do.\n",
    "        self.encoder = eve.cores.PoissonEncoder(timesteps=timesteps)\n",
    "        \n",
    "        # You can define any nn.Module in Eve, Eve can handle them well.\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=2, padding=1),\n",
    "            torch.nn.BatchNorm2d(3),\n",
    "            # torch.nn.ReLU(), # move the ReLU to Node already\n",
    "        )\n",
    "        \n",
    "        state = eve.cores.State(self.conv) # get some necessary argments to define a quan layer\n",
    "        \n",
    "        # add a IfNode\n",
    "        self.ifnode = eve.cores.IfNode(state=state, time_independent=False)\n",
    "        \n",
    "        # add a ste layer to convert the voltage into spiking signals.\n",
    "        self.ste = eve.cores.SteQuan(state=state, max_bits=1)\n",
    "        self.linear = torch.nn.Linear(14 * 14 * 3, 10)\n",
    "        \n",
    "        self.spike() # turn on spiking mode\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # NOTE: spiking neural network contains hidden states of voltage we should reset it every time.\n",
    "        self.reset()\n",
    "        \n",
    "        res = []\n",
    "        for i in range(self.timesteps):\n",
    "            conv = self.conv(x)\n",
    "            \n",
    "            ifnode = self.ifnode(conv)\n",
    "            \n",
    "            # insert ste layer here\n",
    "            ste = self.ste(ifnode)\n",
    "            \n",
    "            ste = torch.flatten(ste, 1)\n",
    "            linear = self.linear(ste)\n",
    "            res.append(linear)\n",
    "            \n",
    "        res = torch.stack(res, dim=0).mean(dim=0)\n",
    "\n",
    "        return torch.nn.functional.log_softmax(res, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define SNN toy model and optimizer, move them to cuda if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda\n"
     ]
    }
   ],
   "source": [
    "max_timesteps = 3\n",
    "snn_toy_model = SnnToyModel(max_timesteps)\n",
    "\n",
    "if use_cuda:\n",
    "    snn_toy_model.cuda()\n",
    "    print(\"use cuda\")\n",
    "else:\n",
    "    print(\"use cpu\")\n",
    "    \n",
    "optimizer = torch.optim.Adam(snn_toy_model.torch_parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train snn_toy_model in dataset for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc on Epoch 1/10 is 89.97%\n",
      "Acc on Epoch 2/10 is 90.89%\n",
      "Acc on Epoch 3/10 is 91.43%\n",
      "Acc on Epoch 4/10 is 91.69%\n",
      "Acc on Epoch 5/10 is 91.95%\n",
      "Acc on Epoch 6/10 is 92.21%\n",
      "Acc on Epoch 7/10 is 92.49%\n",
      "Acc on Epoch 8/10 is 92.65%\n",
      "Acc on Epoch 9/10 is 92.76%\n",
      "Acc on Epoch 10/10 is 92.59%\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    snn_toy_model.train()\n",
    "    for data, target in train_dataloader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.nn.functional.cross_entropy(snn_toy_model(data), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    correct = 0.0\n",
    "    snn_toy_model.eval()\n",
    "    for data, target in test_dataloader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        predict = snn_toy_model(data)\n",
    "        correct += (predict.max(dim=1)[1] == target).float().sum()\n",
    "    print(f\"Acc on Epoch {i+1}/10 is {100 * correct/len(test_dataset):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the performance of spiking neural network is still have a large space to be improved. \n",
    "At this example, you can increase the max_timesteps to increase the final accuracy.\n",
    "\n",
    "\n",
    "You can fetch all the hidden states of **Eve** via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ifnode.voltage_hid torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "for k, hid in snn_toy_model.named_hidden_states():\n",
    "    print(k, torch.typename(hid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quan\n",
    "\n",
    "The following cells, we use the trainer provied by **Eve** to study on the different quantization methods' performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SteQuan\n",
    "\n",
    "SteQuan is the most widely used quantization function with fixed alpha parameters.\n",
    "\n",
    "SteQuan is more stable and can be trained with a larger learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "load pretrained None failed.\n",
      "'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.\n",
      "bit_width reset to 8.\n",
      "set baseline acc as 0.0\n",
      "SteQuan\n",
      "Acc on Epoch 1/10 is 89.87%\n",
      "Acc on Epoch 2/10 is 90.68%\n",
      "Acc on Epoch 3/10 is 91.19%\n",
      "Acc on Epoch 4/10 is 91.60%\n",
      "Acc on Epoch 5/10 is 91.69%\n",
      "Acc on Epoch 6/10 is 91.49%\n",
      "Acc on Epoch 7/10 is 91.79%\n",
      "Acc on Epoch 8/10 is 91.88%\n",
      "Acc on Epoch 9/10 is 91.74%\n",
      "Acc on Epoch 10/10 is 92.05%\n"
     ]
    }
   ],
   "source": [
    "import eve\n",
    "import eve.app\n",
    "from gym.envs import make, spec, registry\n",
    "\n",
    "mnist_trainer = make(\"mnist-v0\",\n",
    "               eve_net_kwargs={\n",
    "                   \"node\": \"IfNode\",\n",
    "                   \"node_kwargs\": {\n",
    "                       \"voltage_threshold\": 0.5,\n",
    "                       \"time_independent\": True,\n",
    "                       \"requires_upgrade\": True,\n",
    "                   },\n",
    "                   \"quan\": \"SteQuan\",\n",
    "                   \"quan_kwargs\": {\n",
    "                       \"max_bits\": 8,\n",
    "                       \"requires_upgrade\": True,\n",
    "                   },\n",
    "                   \"encoder\": \"RateEncoder\",\n",
    "                   \"encoder_kwargs\": {\n",
    "                       \"timesteps\": 1,\n",
    "                   }\n",
    "               },\n",
    "               max_bits=8,\n",
    "               root_dir=\"/media/densechen/data/code/eve-mli/examples/logs\",\n",
    "               data_root=\"/media/densechen/data/dataset\",\n",
    "               pretrained=None,\n",
    "               device=\"auto\")\n",
    "print(\"SteQuan\")\n",
    "for i in range(10):\n",
    "    mnist_trainer.train_one_epoch()\n",
    "    acc = mnist_trainer.test_one_epoch()[\"acc\"]\n",
    "    print(f\"Acc on Epoch {i+1}/10 is {100 * acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LsqQuan\n",
    "\n",
    "LsqQuan has a trainable alpha parameter, which is supervised with global error.\n",
    "It is unstable during training and prunes to make a invalid alpha value, which is not a positive one.\n",
    "Using a smaller learning rate is vital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "load pretrained None failed.\n",
      "'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.\n",
      "bit_width reset to 8.\n",
      "set baseline acc as 0.0\n",
      "LsqQuan\n",
      "Acc on Epoch 1/10 is 91.54%\n",
      "Acc on Epoch 2/10 is 92.33%\n",
      "Acc on Epoch 3/10 is 93.20%\n",
      "Acc on Epoch 4/10 is 93.78%\n",
      "Acc on Epoch 5/10 is 93.99%\n",
      "Acc on Epoch 6/10 is 90.55%\n",
      "Acc on Epoch 7/10 is 81.58%\n",
      "Acc on Epoch 8/10 is 16.22%\n",
      "Acc on Epoch 9/10 is 11.36%\n",
      "Acc on Epoch 10/10 is 11.36%\n"
     ]
    }
   ],
   "source": [
    "import eve\n",
    "import eve.app\n",
    "from gym.envs import make, spec, registry\n",
    "\n",
    "mnist_trainer = make(\"mnist-v0\",\n",
    "               eve_net_kwargs={\n",
    "                   \"node\": \"IfNode\",\n",
    "                   \"node_kwargs\": {\n",
    "                       \"voltage_threshold\": 0.5,\n",
    "                       \"time_independent\": True,\n",
    "                       \"requires_upgrade\": True,\n",
    "                   },\n",
    "                   \"quan\": \"LsqQuan\",\n",
    "                   \"quan_kwargs\": {\n",
    "                       \"max_bits\": 8,\n",
    "                       \"requires_upgrade\": True,\n",
    "                   },\n",
    "                   \"encoder\": \"RateEncoder\",\n",
    "                   \"encoder_kwargs\": {\n",
    "                       \"timesteps\": 1,\n",
    "                   }\n",
    "               },\n",
    "               max_bits=8,\n",
    "               root_dir=\"/media/densechen/data/code/eve-mli/examples/logs\",\n",
    "               data_root=\"/media/densechen/data/dataset\",\n",
    "               pretrained=None,\n",
    "               device=\"auto\")\n",
    "print(\"LsqQuan\")\n",
    "for i in range(10):\n",
    "    mnist_trainer.train_one_epoch()\n",
    "    acc = mnist_trainer.test_one_epoch()[\"acc\"]\n",
    "    print(f\"Acc on Epoch {i+1}/10 is {100 * acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlsqQuan\n",
    "\n",
    "LlsqQuan has a trainable alpha parameter, which is supervised with local error. It is more stable than LsqQuan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "load pretrained None failed.\n",
      "'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.\n",
      "bit_width reset to 8.\n",
      "set baseline acc as 0.0\n",
      "LlsqQuan\n",
      "Acc on Epoch 1/10 is 93.25%\n",
      "Acc on Epoch 2/10 is 94.65%\n",
      "Acc on Epoch 3/10 is 95.56%\n",
      "Acc on Epoch 4/10 is 95.93%\n",
      "Acc on Epoch 5/10 is 96.39%\n",
      "Acc on Epoch 6/10 is 96.28%\n",
      "Acc on Epoch 7/10 is 96.56%\n",
      "Acc on Epoch 8/10 is 96.68%\n",
      "Acc on Epoch 9/10 is 96.73%\n",
      "Acc on Epoch 10/10 is 96.67%\n"
     ]
    }
   ],
   "source": [
    "import eve\n",
    "import eve.app\n",
    "from gym.envs import make, spec, registry\n",
    "\n",
    "mnist_trainer = make(\"mnist-v0\",\n",
    "               eve_net_kwargs={\n",
    "                   \"node\": \"IfNode\",\n",
    "                   \"node_kwargs\": {\n",
    "                       \"voltage_threshold\": 0.5,\n",
    "                       \"time_independent\": True,\n",
    "                       \"requires_upgrade\": True,\n",
    "                   },\n",
    "                   \"quan\": \"LlsqQuan\",\n",
    "                   \"quan_kwargs\": {\n",
    "                       \"max_bits\": 8,\n",
    "                       \"requires_upgrade\": True,\n",
    "                   },\n",
    "                   \"encoder\": \"RateEncoder\",\n",
    "                   \"encoder_kwargs\": {\n",
    "                       \"timesteps\": 1,\n",
    "                   }\n",
    "               },\n",
    "               max_bits=8,\n",
    "               root_dir=\"/media/densechen/data/code/eve-mli/examples/logs\",\n",
    "               data_root=\"/media/densechen/data/dataset\",\n",
    "               pretrained=None,\n",
    "               device=\"auto\")\n",
    "print(\"LlsqQuan\")\n",
    "for i in range(10):\n",
    "    mnist_trainer.train_one_epoch()\n",
    "    acc = mnist_trainer.test_one_epoch()[\"acc\"]\n",
    "    print(f\"Acc on Epoch {i+1}/10 is {100 * acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Spiking and Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "load pretrained None failed.\n",
      "'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.\n",
      "bit_width reset to 1.\n",
      "set baseline acc as 0.0\n",
      "QuanSpiking\n",
      "Acc on Epoch 1/10 is 89.67%\n",
      "Acc on Epoch 2/10 is 91.16%\n",
      "Acc on Epoch 3/10 is 91.38%\n",
      "Acc on Epoch 4/10 is 91.48%\n",
      "Acc on Epoch 5/10 is 91.55%\n",
      "Acc on Epoch 6/10 is 91.46%\n",
      "Acc on Epoch 7/10 is 91.03%\n",
      "Acc on Epoch 8/10 is 91.40%\n",
      "Acc on Epoch 9/10 is 91.67%\n",
      "Acc on Epoch 10/10 is 91.75%\n"
     ]
    }
   ],
   "source": [
    "import eve\n",
    "import eve.app\n",
    "from gym.envs import make, spec, registry\n",
    "\n",
    "mnist_trainer = make(\"mnist-v0\",\n",
    "               eve_net_kwargs={\n",
    "                   \"node\": \"IfNode\",\n",
    "                   \"node_kwargs\": {\n",
    "                       \"voltage_threshold\": 0.5,\n",
    "                       \"time_independent\": True,\n",
    "                       \"requires_upgrade\": True,\n",
    "                   },\n",
    "                   \"quan\": \"SteQuan\",\n",
    "                   \"quan_kwargs\": {\n",
    "                       \"requires_upgrade\": True,\n",
    "                   },\n",
    "                   \"encoder\": \"RateEncoder\",\n",
    "                   \"encoder_kwargs\": {\n",
    "                       \"timesteps\": 1,\n",
    "                   }\n",
    "               },\n",
    "               max_bits=1,\n",
    "               root_dir=\"/media/densechen/data/code/eve-mli/examples/logs\",\n",
    "               data_root=\"/media/densechen/data/dataset\",\n",
    "               pretrained=None,\n",
    "               device=\"auto\")\n",
    "print(\"QuanSpiking\")\n",
    "mnist_trainer.eve_net.spike()\n",
    "for i in range(10):\n",
    "    mnist_trainer.train_one_epoch()\n",
    "    acc = mnist_trainer.test_one_epoch()[\"acc\"]\n",
    "    print(f\"Acc on Epoch {i+1}/10 is {100 * acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
